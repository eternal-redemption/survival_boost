#xgb_res=c()
#gbm_res=c()

#for(k in 1:10){
  n1=500
  n2=500
  n=n1+n2
  p=50
  #beta=1
  rho=0.75; #0.75
  
  #generate covaraince matrix  V=rho^|i-j|
  #we can also generate R then use cholesky to intrdouce cor.
  V=matrix(0,ncol=p,nrow=p)
  for (i in 1:p) {
    for (j in 1:p ){
      V[i,j]=rho^abs(i-j)
    }
  }
  X=MASS::mvrnorm(n=n,mu=rep(0,p),Sigma=V)
  #X=matrix(runif(n*p,-3,3),nrow=n,byrow=T)
  #mu=exp(2*pnorm(X[,10]^2+X[,50]^2-1)) #***
  #mu=exp(2*pnorm(X[,30]*X[,10]+X[,50]^2+X[,10]^2-1))
  #mu=exp(2*pnorm(sin(X[,10])+X[,50]^2-1))
  #mu=exp(2*pnorm(cos(X[,10])+X[,50]^2-1)) #***
  #mu=exp(2*pnorm((X[,10]>0.5)+X[,50]^2-1)) #***
  #mu=exp(-1+X[,20]^2/0.5+X[,40]^2/0.5+X[,30]^2/0.5+X[,10]^2/0.5+X[,50]^2/0.5)
  
  #mu=2*pnorm(-1+X[,10]*X[,30]+X[,50]^2)  # 2*pnorm()
  #2.5*pnorm(-1+X[,20]^2/1+X[,50]^2/1+X[,30]*X[,10]+sin(X[,5])+(X[,15]>0)+sin(6*X[,2])+
  #           4*X[,4]^3+cos(6*X[,3])+X[,25]^2*X[,45]^2)
  mu=0
  #for(i in seq(5,p,5)){
  for(i in seq(1,p*0.2,1)){
    if((i/1)%%4==0){mu=mu+X[,i]^2/0.5}
    if((i/1)%%4==1){mu=mu+cos(X[,i])}
    if((i/1)%%4==2){mu=mu+sin(i*X[,i])}
    if((i/1)%%4==3){mu=mu+X[,i]*X[,i-1]}
  }
  #mu=exp(-p)*exp(mu) #0.05 FOR 50
  #mu=exp((X[,10]>0.8)-1) #*** try this plot(gbm1,c(10,30),best.iter)
  mu=0.1*abs(mu) 
  #T=-(log(runif(n)))/(mu)
  #T=(rexp(n,5))/(mu)
  T=rweibull(n, shape=2, scale=mu)
  #a=2*rbinom(n=n, size=1, prob=1/3); b=runif(n=n,min=0,max=2)
  #a[a==0]=b[a==0]
  #C=a
  C=runif(n, min = 0, max = 2)
  C[sample(n,n/3)]=2
  obs.time<- pmin(T,C)
  status <- T<=C
  #table(status)
  library(ggplot2)
  library("survival")
  library('survminer')
  library(survcomp)
  fit1 <- survfit(Surv(obs.time,status)~1)
  plot(fit1)
  
  ggsurvplot(fit1,risk.table = TRUE)
  
  #coxph
  require(survival)
  df2 <- structure(list(X_c=X[1:n1,], status_c=status[1:n1], obs.time_c=obs.time[1:n1], class = "data.frame"))
  fit1=coxph(Surv(obs.time_c, status_c)~ X_c, method="breslow",data=df2)
  newdata=structure(list(X_c=X[(n1+1):n,], 
                         .Names = c("X_c"), class = "data.frame"))
  cox_pred=predict(fit1,newdata,type="lp")
  #gbm cox
  library(gbm)
  library(survival)
  gbm1 <- gbm(Surv(obs.time[1:n1],status[1:n1])~ .,       # formula
              data=as.data.frame(X[1:n1,]),                 # dataset
              #weights=w,
              #var.monotone=c(0,0,0),     # -1: monotone decrease, +1: monotone increase, 0: no monotone restrictions
              distribution="coxph",
              n.trees=2000,              # number of trees
              shrinkage=0.005,           # shrinkage or learning rate, 0.001 to 0.1 usually work
              #interaction.depth=1,       # 1: additive model, 2: two-way interactions, etc
              bag.fraction = 0.5,        # subsampling fraction, 0.5 is probably best
              train.fraction = 1,      # fraction of data for training, first train.fraction*N used for training
              cv.folds = 5,              # do 5-fold cross-validation
              #n.minobsinnode = 10,       # minimum total weight needed in each node
              keep.data = TRUE,
              verbose = TRUE)           #  print progress
  #summary(gbm1)
  ?gbm
  best.iter <- gbm.perf(gbm1,method="cv")
  ss=summary(gbm1,n.trees=best.iter) # based on the estimated best number of trees
  ss$rel.inf
  ss$var
  gbm_pred=predict(gbm1,as.data.frame(X[(n1+1):n,]))
  survConcordance(Surv(obs.time[(n1+1):n], status[(n1+1):n]) ~ gbm_pred)
  
  
  library(xgboost)
  Dtrain<-xgb.DMatrix(X[1:n1,],label=obs.time[1:n1]*(-(-1)^(as.numeric(status[1:n1]))))
  Dtest<-xgb.DMatrix(X[(n1+1):n,],label=obs.time[(n1+1):n]*(-(-1)^(as.numeric(status[(n1+1):n]))))
  
  mylossobj2<-function(preds, dtrain) {
    labels <- getinfo(dtrain, "label") 
    #censor<-attr(dtrain,"censor")
    censor= labels>0
    labels=abs(labels)
    ord<-order(labels)
    ran=rank(labels)
    d=censor[ord]  #status
    etas=preds[ord] #linear predictor
    haz<-as.numeric(exp(etas)) #w[i]
    rsk<-rev(cumsum(rev(haz))) #W[i]
    P<-outer (haz,rsk,'/')
    P[upper.tri(P)] <- 0
    grad<- -(d-P%*%d)
    grad=grad[ran]
    H1=P
    H2=outer(haz^2,rsk^2,'/')
    H=H1-H2
    H[upper.tri(H)]=0
    hess=H%*%d
    hess=hess[ran]
    return(list(grad = grad, hess = hess))
  }
  
  #evalerror2 <- function(preds, dtrain) {
  #  labels <- getinfo(dtrain, "label") 
  #  censor= labels>0
  #  labels=abs(labels)
  #  err <- as.numeric(survConcordance(Surv(labels, censor) ~ preds)$concordance)
  #  return(list(metric = "cindex",value = -err))
  #}
  evalerror2 <- function(preds, dtrain) {
    labels <- getinfo(dtrain, "label") #labels<-dtrain$label
    censor= labels>0
    labels=abs(labels)
    ord<-order(labels)
    d=censor[ord]  #status
    etas=preds[ord] #linear predictor
    haz<-as.numeric(exp(etas)) #w[i]
    rsk<-rev(cumsum(rev(haz)))
    err <- -2*sum(d*(etas-log(rsk)))/length(labels)
    return(list(metric = "cindex",value = err))
  }
  
  #xgb_res[k]=as.numeric((survConcordance(Surv(obs.time[(n1+1):n], status[(n1+1):n]) ~ xgb_pred))$concordance)
  #gbm_res[k]=as.numeric((survConcordance(Surv(obs.time[(n1+1):n], status[(n1+1):n]) ~ gbm_pred))$concordance)
  #survConcordance(Surv(obs.time[(n1+1):n], status[(n1+1):n]) ~ gbm_pred)
#}


#mean(xgb_res)
#mean(gbm_res)












###########################   parameter tuning   #######################
res=NA
best_param = list()
best_seednumber = 1234
best_loss = Inf
best_loss_index = 0

for (iter in 1:100) {
  param <- list(objective = mylossobj2,
                eval_metric = evalerror2,
                #num_class = 12,
                max_depth = sample(3:10, 1),
                #eta =runif(1, .01, .1),
                eta=runif(1, .001, .01),
                gamma = runif(1, 0.0, 0.05), 
                subsample = 0.5, #runif(1, .5, 1),
                #colsample_bytree = runif(1, .5,1), 
                min_child_weight = sample(1:20, 1),
                max_delta_step = sample(1:10, 1),
                #colsample_bylevel=runif(1, .5, 1),
                lambda=runif(1,0,2),
                alpha=runif(1,0,5)
  )
  cv.nround = 2000
  cv.nfold = 5
  seed.number = sample.int(10000, 1)[[1]]
  set.seed(seed.number)
  mdcv <- xgb.cv(data=Dtrain, params = param, nthread=6, 
                 nfold=cv.nfold, nrounds=cv.nround,
                 verbose = F)
  
  min_loss = min(mdcv$evaluation_log[,'test_cindex_mean'])
  min_loss_index = which.min(as.numeric(unlist(mdcv$evaluation_log[,'test_cindex_mean'])))
  
  if (min_loss < best_loss) {
    best_loss = min_loss
    best_loss_index = min_loss_index
    best_seednumber = seed.number
    best_param = param
  }
  print(iter)
}

nround = best_loss_index
set.seed(best_seednumber)
best_param$objective=mylossobj2
md <- xgboost(data=Dtrain, params=best_param, nrounds=nround,nthread=6)
a=xgb.importance(model=md)

xgb_pred=predict(md,Dtest)
#res=list(X,obs.time,status,gbm1,gbm_pred,fit1,cox_pred,a,xgb_pred)
#save(res,file=paste("/home/xw75/zhenyu/", OUTNAME+i, ".Rdata", sep="" ))
survConcordance(Surv(obs.time[(n1+1):n], status[(n1+1):n]) ~ xgb_pred)
survConcordance(Surv(obs.time[(n1+1):n], status[(n1+1):n]) ~ cox_pred)
survConcordance(Surv(obs.time[(n1+1):n], status[(n1+1):n]) ~ gbm_pred)

dd_xgb=data.frame("time"=obs.time[(n1+1):n],"event"=status[(n1+1):n],"score"=(xgb_pred))
sbrier.score2proba(data.tr=dd_xgb, data.ts = dd_xgb, method = "cox")$bsc.integrated
dd_gbm=data.frame("time"=obs.time[(n1+1):n],"event"=status[(n1+1):n],"score"=(gbm_pred))
sbrier.score2proba(data.tr=dd_gbm, data.ts = dd_gbm, method = "cox")$bsc.integrated



